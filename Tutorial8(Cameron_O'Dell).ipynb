{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIIUsfCxdyBx"
      },
      "source": [
        "### TASK I\n",
        "\n",
        "The ECT for this TASK is 30 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgEXHzXGeSPY"
      },
      "source": [
        "Report the results after changing the input query in the below code. In the **Tutorial Completion Document**, record your observations (what will be the output for the new query input)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace `{API_TOKEN}` with your token created in Activity-II."
      ],
      "metadata": {
        "id": "jfike7HoY5kA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "roYAXOlafGUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e6f663d-db6d-4974-d4b3-41cf0d6b6942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"How many km from Earth to the Moon? I'm assuming it's the same number and distance as Jupiter's moon Europa.\\n\\nThe size of Jupiter was calculated for each year, starting with the 1980-2001 year to find the planet's size\"}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "API_TOKEN = \"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\"\n",
        "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "\n",
        "def query(payload):\n",
        "    data = json.dumps(payload)\n",
        "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
        "    return json.loads(response.content.decode(\"utf-8\"))\n",
        "\n",
        "# Example usage\n",
        "data = query({\"inputs\": \"How many km from Earth to the Moon?\"})\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "fpath = 'gdrive/My Drive/Colab Notebooks/Tutorial 8/'  #change dir to your project folder or Image folder"
      ],
      "metadata": {
        "id": "NGln01EqYxK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75871d5d-8580-4639-cd1f-250d31c57c7f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "API_TOKEN = \"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\"\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/vit-base-patch16-224\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "def query(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
        "    return json.loads(response.content.decode(\"utf-8\"))\n",
        "data = query(fpath+\"data/Cat1.jpg\")\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0aae1ac-e84b-4831-ceb3-f85a8c0ad808",
        "id": "Imghm0j2UXDt"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.44724124670028687, 'label': 'tabby, tabby cat'}, {'score': 0.4454857409000397, 'label': 'Egyptian cat'}, {'score': 0.0954064130783081, 'label': 'tiger cat'}, {'score': 0.003450191579759121, 'label': 'lynx, catamount'}, {'score': 0.0011419756338000298, 'label': 'Persian cat'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK I Findings\n",
        "\n",
        "In Task I generated an API Key on Hugging Face and created a query using the input \"How many km from Earth to the Moon?\". The model returned a response. The response was mostly nonsensical. It was tangentially related since its response was about Jupiter and Europa. However, the answer was imprecise.\n",
        "\n",
        "Continuing with Task I, I created a folder in my google drive and uploaded the images contained in the data file for Tutorial 8. \n",
        "\n",
        "Using a new model, vit-base (Vision Transformer), I queried the Cat1.jpg with the model. This returned that it was most likely a Tabby Cat or Egpytian Cat with approximate scores of 0.45 each (rounded). I'm not the most familiar with different breeds of cats . However, comparing Cat1.jpg to images online it does seem like it could be one of those breeds. The score of 0.45 represents a moderate level of confidence that the image matches the label.\n",
        "\n",
        "Other cat breeds were listed for labels but the confidence was substantially lower (close to zero). This indicates that the model is confident that the other labels do not provide a good match."
      ],
      "metadata": {
        "id": "Q0wHqXoBjszu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx8w58kgbFiH"
      },
      "source": [
        "### TASK II\n",
        "\n",
        "The ECT for this TASK is 60 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEC-yzG8bFiH"
      },
      "source": [
        "a. Report the results after changing the input images to the other images we have provided in the \"data\" folder. In the **Tutorial Completion Document**, record your observations (what will be the output label for the new images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz5LDYvlbFiH",
        "outputId": "1e76b71c-f1db-4d04-8a89-91788060ddd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.4468021094799042, 'label': 'tabby, tabby cat'}, {'score': 0.17969028651714325, 'label': 'Egyptian cat'}, {'score': 0.10806704312562943, 'label': 'Persian cat'}, {'score': 0.0954844057559967, 'label': 'tiger cat'}, {'score': 0.058583371341228485, 'label': 'lynx, catamount'}]\n",
            "[{'score': 0.24109460413455963, 'label': 'tabby, tabby cat'}, {'score': 0.19858786463737488, 'label': 'Egyptian cat'}, {'score': 0.16881126165390015, 'label': 'carton'}, {'score': 0.06312225013971329, 'label': 'tiger cat'}, {'score': 0.037279125303030014, 'label': 'space heater'}]\n",
            "[{'score': 0.9588029384613037, 'label': 'golden retriever'}, {'score': 0.01523493230342865, 'label': 'Labrador retriever'}, {'score': 0.005649607628583908, 'label': 'Sussex spaniel'}, {'score': 0.0017612158553674817, 'label': 'otterhound, otter hound'}, {'score': 0.001748998067341745, 'label': 'kuvasz'}]\n",
            "[{'score': 0.9078043699264526, 'label': 'pug, pug-dog'}, {'score': 0.052073102444410324, 'label': 'Brabancon griffon'}, {'score': 0.011947802267968655, 'label': 'bull mastiff'}, {'score': 0.0033383246045559645, 'label': 'Pekinese, Pekingese, Peke'}, {'score': 0.002020379062741995, 'label': 'Japanese spaniel'}]\n",
            "[{'score': 0.9915756583213806, 'label': 'daisy'}, {'score': 0.0009958477457985282, 'label': 'bolo tie, bolo, bola tie, bola'}, {'score': 0.00011169931531185284, 'label': 'vase'}, {'score': 0.000108190237369854, 'label': 'hair slide'}, {'score': 0.00010748156637419015, 'label': 'pot, flowerpot'}]\n",
            "[{'score': 0.6797338724136353, 'label': 'cardoon'}, {'score': 0.0572655089199543, 'label': 'daisy'}, {'score': 0.013612658716738224, 'label': 'artichoke, globe artichoke'}, {'score': 0.007020329590886831, 'label': 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus'}, {'score': 0.006049599498510361, 'label': 'bee'}]\n",
            "[{'score': 0.4766232967376709, 'label': 'valley, vale'}, {'score': 0.44562044739723206, 'label': 'alp'}, {'score': 0.01942760869860649, 'label': 'mountain tent'}, {'score': 0.014382691122591496, 'label': 'volcano'}, {'score': 0.003581519704312086, 'label': 'lakeside, lakeshore'}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import requests\n",
        "API_TOKEN = \"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\"\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/vit-base-patch16-224\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "def query(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
        "    return json.loads(response.content.decode(\"utf-8\"))\n",
        "data_cat2 = query(fpath+\"data/Cat2.jpg\")\n",
        "data_cat3 = query(fpath+\"data/Cat3.jpg\")\n",
        "data_dog1 = query(fpath+\"data/Dog1.jpg\")\n",
        "data_dog3 = query(fpath+\"data/Dog3.jpg\")\n",
        "data_flower1 = query(fpath+\"data/Flower1.jpg\")\n",
        "data_flower3 = query(fpath+\"data/Flower3.jpg\")\n",
        "data_mountain1= query(fpath+\"data/Mountain1.jpg\")\n",
        "print(data_cat2)\n",
        "print(data_cat3)\n",
        "print(data_dog1)\n",
        "print(data_dog3)\n",
        "print(data_flower1)\n",
        "print(data_flower3)\n",
        "print(data_mountain1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK IIa Findings\n",
        "\n",
        "In Task II, part a I queried the all of the images in the data folder and included an additional image of a mountain. Similar to Task I, a score was generated for each image with respective labels. I noticed the Cat3.jpg had a low degree of confidence for what kind of cat was in the picture. I'm guessing this is because the image is cropped and not a complete image of the cat. Or it could be that the cat is a mixed breed, so its having a trouble neatly identifying which breed it is.\n",
        "\n",
        "The mountain image I included was identified as a valley and an alp both with a score of around 0.45. The picture includes a mountain with a valley in the foreground so that would align with the image context.\n"
      ],
      "metadata": {
        "id": "rdjpS68Jsd8Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS7BeFFV_q7_"
      },
      "source": [
        "b. Report the results after changing the input images to the other images we have provided in the \"data\" folder and changing the model from `\"google/vit-base-patch16-224\"` to `\"facebook/detr-resnet-50\"`. In the **Tutorial Completion Document**, record your observations (what will be the output label and scores for the new images with the new model)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "API_TOKEN = \"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\"\n",
        "API_URL = \"https://api-inference.huggingface.co/models/facebook/detr-resnet-50\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "def query(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
        "    return json.loads(response.content.decode(\"utf-8\"))\n",
        "data_cat2 = query(fpath+\"data/Cat2.jpg\")\n",
        "data_cat3 = query(fpath+\"data/Cat3.jpg\")\n",
        "data_dog1 = query(fpath+\"data/Dog1.jpg\")\n",
        "data_dog3 = query(fpath+\"data/Dog3.jpg\")\n",
        "data_flower1 = query(fpath+\"data/Flower1.jpg\")\n",
        "data_flower3 = query(fpath+\"data/Flower3.jpg\")\n",
        "data_mountain1 = query(fpath+\"data/Mountain1.jpg\")\n",
        "print(data_cat2)\n",
        "print(data_cat3)\n",
        "print(data_dog1)\n",
        "print(data_dog3)\n",
        "print(data_flower1)\n",
        "print(data_flower3)\n",
        "print(data_mountain1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f60d3XBNaXpb",
        "outputId": "3c797907-c90e-44ad-e0f7-024ea269213f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.9979555606842041, 'label': 'cat', 'box': {'xmin': 0, 'ymin': 0, 'xmax': 349, 'ymax': 300}}]\n",
            "[{'score': 0.9920125603675842, 'label': 'cat', 'box': {'xmin': 0, 'ymin': 0, 'xmax': 136, 'ymax': 182}}]\n",
            "[{'score': 0.9880869388580322, 'label': 'dog', 'box': {'xmin': 4, 'ymin': 2, 'xmax': 86, 'ymax': 144}}]\n",
            "[{'score': 0.9823790192604065, 'label': 'dog', 'box': {'xmin': 25, 'ymin': 5, 'xmax': 148, 'ymax': 115}}]\n",
            "[{'score': 0.9147478342056274, 'label': 'umbrella', 'box': {'xmin': 67, 'ymin': 10, 'xmax': 161, 'ymax': 101}}]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK IIb Findings\n",
        "\n",
        "In Task II, part b the Facebook Detection Transformer is used instead of VIT. Upon querying the images, it appears that the labels are much more simplistic. However, there is a higher degree of certain by the model in the category types. This model trades off specifics for accuracy. \n",
        "\n",
        "The Flower3.jpg image was misidentified as an umbrella. I'm not sure why, but it could be something to do with the angle of the image. It also looks less photorealistic/lower resolution. "
      ],
      "metadata": {
        "id": "tltX0L4N9uvO"
      }
    }
  ]
}